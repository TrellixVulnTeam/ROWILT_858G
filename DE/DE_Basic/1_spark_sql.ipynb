{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read csv file using Apache Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version check\n",
    "# !which python\n",
    "!/opt/conda/bin/python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark version check\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file (no options: default column names & default data type(String))\n",
    "log_access = spark.read.csv(\"data/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read.option(\"head\", \"true\") : get column names\n",
    "log_access = spark.read.option(\"header\", \"true\").csv(\"data/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read.option(\"inferSchema\", \"true\") : spark infers the types of data\n",
    "log_access = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of 2 method types of spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) calling structured API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date, col, lit\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", \"true\").json(\"data/activity-data\")\n",
    "\n",
    "timestamp = df.select(\n",
    "    \"Arrival_Time\",\n",
    "    to_timestamp(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Datetime'),\n",
    "    to_date(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd').alias('String_Date')\n",
    ")\n",
    "timestamp.show(5)\n",
    "# lit() : function to add constant column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) print using SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQL expression\n",
    "ts = df.selectExpr(\n",
    "    \"Arrival_Time\",\n",
    "    \"to_timestamp(from_unixtime(Arrival_Time / 1000), 'yyyy-MM-dd HH:mm:ss') as String_Datetime\",\n",
    "    \"to_date(from_unixtime(Arrival_Time / 1000), 'yyyy-MM-dd') as String_Date\"\n",
    ")\n",
    "ts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"index\") > 100).select(\"index\", \"user\").groupBy(\"user\").count().show() \n",
    "df.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read json file using Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read.json\n",
    "json = spark.read.json(\"data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")\n",
    "users = json.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & lookup view table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary view table that can lookup data in only current session\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"select * from users where count is not null and count > 9000 order by count desc\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 types of reading json file\n",
    "\n",
    "df = spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()\n",
    "\n",
    "df2 = spark.read.load(\"./data/flight-data/json/2015-summary.json\", format=\"json\")\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = spark.read.json(\"./data/flight-data/json/2015-summary.json\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex 1\n",
    "df1 = spark.read.csv(\"data/tbl_user.csv\")\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex 2\n",
    "df2 = spark.read.option(\"header\", \"true\").csv(\"data/tbl_purchase.csv\")\n",
    "df2.show(5)\n",
    "df2_new = df2.select(\n",
    "    to_date(from_unixtime(col('p_time') / 1000), 'yyyy-MM-dd').alias(\"p_time\"),\n",
    "    \"p_uid\", \"p_id\", \"p_name\", \"p_amount\"\n",
    ")\n",
    "df2_new.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
