# Deep Learning Issues
### Loss function
- L1 Loss
- L2 Loss
- Cross Entropy Loss
- Huber Loss
- Hinge Loss
- ...

### Optimizer
- SGD
- Adam
- Adagrad
- Adadelta
- RMSprop
- ...

### Activation : For non-linearity
- Softmax
- Sigmoid : Disadvantage of small change at both extremes
- ReLu

###
