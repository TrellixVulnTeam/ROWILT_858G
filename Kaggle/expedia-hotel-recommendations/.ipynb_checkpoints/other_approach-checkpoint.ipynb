{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37670293 entries, 0 to 37670292\n",
      "Data columns (total 26 columns):\n",
      "date_time                    int64\n",
      "site_name                    int64\n",
      "posa_continent               int64\n",
      "user_location_country        int64\n",
      "user_location_region         int64\n",
      "user_location_city           int64\n",
      "orig_destination_distance    float64\n",
      "user_id                      int64\n",
      "is_mobile                    int64\n",
      "is_package                   int64\n",
      "channel                      int64\n",
      "srch_ci                      object\n",
      "srch_co                      object\n",
      "srch_adults_cnt              int64\n",
      "srch_children_cnt            int64\n",
      "srch_rm_cnt                  int64\n",
      "srch_destination_id          int64\n",
      "srch_destination_type_id     int64\n",
      "is_booking                   int64\n",
      "cnt                          int64\n",
      "hotel_continent              int64\n",
      "hotel_country                int64\n",
      "hotel_market                 int64\n",
      "hotel_cluster                int64\n",
      "year                         int64\n",
      "month                        int64\n",
      "dtypes: float64(1), int64(23), object(2)\n",
      "memory usage: 7.3+ GB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"date_time\"] = pd.to_datetime(all_data[\"date_time\"])\n",
    "all_data[\"year\"] = all_data[\"date_time\"].dt.year\n",
    "all_data[\"month\"] = all_data[\"date_time\"].dt.month\n",
    "all_data[\"date_time\"] = all_data[\"date_time\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"srch_ci\"] = pd.to_datetime(all_data[\"srch_ci\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "all_data[\"srch_ci_year\"] = all_data[\"srch_ci\"].dt.year\n",
    "all_data[\"srch_ci_month\"] = all_data[\"srch_ci\"].dt.month\n",
    "all_data[\"srch_ci_day\"] = all_data[\"srch_ci\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"srch_co\"] = pd.to_datetime(all_data[\"srch_co\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "all_data[\"srch_co_year\"] = all_data[\"srch_co\"].dt.year\n",
    "all_data[\"srch_co_month\"] = all_data[\"srch_co\"].dt.month\n",
    "all_data[\"srch_co_day\"] = all_data[\"srch_co\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in all_data.keys():\n",
    "    all_data = all_data[pd.notnull(all_data[key])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_id_set = set(all_data['hotel_cluster'])\n",
    "train = None\n",
    "test = None\n",
    "# all_data = all_data[all_data['is_booking'] == 1]\n",
    "# total = 0\n",
    "for hotel_id in hotel_id_set:\n",
    "    flt = all_data[all_data['hotel_cluster'] == hotel_id]\n",
    "    flt = shuffle(flt)\n",
    "    l = len(flt)\n",
    "    train_rows = int(l * 0.7)\n",
    "    if train is None:\n",
    "        train = flt[:train_rows]\n",
    "        test = flt[train_rows:]\n",
    "    else:\n",
    "        train = pd.concat([train, flt[:train_rows]])\n",
    "        test = pd.concat([test, flt[train_rows:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(all_data.shape)\n",
    "train.to_csv('train_naive.csv', index=False)\n",
    "test.to_csv('test_naive.csv', index=False)\n",
    "print(\"csv files written to train_naive.csv, test_naive.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = read_train_test_csv()\n",
    "print(\"train shape: \\n\", train.shape)\n",
    "print(\"test shape: \\n\", test.shape)\n",
    "# print(\"train keys: \\n\", train.keys())\n",
    "# print(train.head(1))\n",
    "# print(\"shape of 30 columns train:\", train.ix[:, 30:].shape)\n",
    "performances = []\n",
    "performances.append([\"Naive Bayes\", classify_gaussian_nb(train, test)])\n",
    "performances.append([\"Random Forest\", classify_random_forest(train, test)])\n",
    "performances.append([\"Logistic Regression\", classify_logistic_regression(train, test)])\n",
    "performances.append([\"Gradient Boosting\", classify_gradient_boosting_classifier(train, test)])\n",
    "for val in performances:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_split_data(all_data):\n",
    "    \"\"\"\n",
    "    reads data from 'train.csv' and splits it evenly into train and test data maintaining a ratio of 70-30\n",
    "    writes the train and test data into csv\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print('reading data...')\n",
    "    # train = pd.read_csv('train.csv')\n",
    "    all_data = pd.read_csv('train.csv')\n",
    "#     all_data = pd.read_csv('out.csv')\n",
    "    print('read')\n",
    "    all_data[\"date_time\"] = pd.to_datetime(all_data[\"date_time\"])\n",
    "    all_data[\"year\"] = all_data[\"date_time\"].dt.year\n",
    "    all_data[\"month\"] = all_data[\"date_time\"].dt.month\n",
    "    all_data[\"date_time\"] = all_data[\"date_time\"].dt.day\n",
    "    print('Reading Done date')\n",
    "    \n",
    "    all_data[\"srch_ci\"] = pd.to_datetime(all_data[\"srch_ci\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "    all_data[\"srch_ci_year\"] = all_data[\"srch_ci\"].dt.year\n",
    "    all_data[\"srch_ci_month\"] = all_data[\"srch_ci\"].dt.month\n",
    "    all_data[\"srch_ci_day\"] = all_data[\"srch_ci\"].dt.day\n",
    "    print('Reading Done srch_ci')\n",
    "    \n",
    "    all_data[\"srch_co\"] = pd.to_datetime(all_data[\"srch_co\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "    all_data[\"srch_co_year\"] = all_data[\"srch_co\"].dt.year\n",
    "    all_data[\"srch_co_month\"] = all_data[\"srch_co\"].dt.month\n",
    "    all_data[\"srch_co_day\"] = all_data[\"srch_co\"].dt.day\n",
    "    \n",
    "    print('Reading Done All')\n",
    "    \n",
    "    for key in all_data.keys():\n",
    "        all_data = all_data[pd.notnull(all_data[key])]\n",
    "\n",
    "    # print(all.keys())\n",
    "    hotel_id_set = set(all_data['hotel_cluster'])\n",
    "    train = None\n",
    "    test = None\n",
    "    # all_data = all_data[all_data['is_booking'] == 1]\n",
    "    # total = 0\n",
    "    for hotel_id in hotel_id_set:\n",
    "        flt = all_data[all_data['hotel_cluster'] == hotel_id]\n",
    "        flt = shuffle(flt)\n",
    "        l = len(flt)\n",
    "        train_rows = int(l * 0.7)\n",
    "        if train is None:\n",
    "            train = flt[:train_rows]\n",
    "            test = flt[train_rows:]\n",
    "        else:\n",
    "            train = pd.concat([train, flt[:train_rows]])\n",
    "            test = pd.concat([test, flt[train_rows:]])\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    print(all_data.shape)\n",
    "    train.to_csv('train_naive.csv', index=False)\n",
    "    test.to_csv('test_naive.csv', index=False)\n",
    "    print(\"csv files written to train_naive.csv, test_naive.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test_csv():\n",
    "    \"\"\"\n",
    "    reads the train_naive and test_naive csv and returns as a tuple\n",
    "    :return: train , test tuple\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\"train_naive.csv\"), pd.read_csv(\"test_naive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_random_forest(train, test):\n",
    "    \"\"\"\n",
    "    classify using random forest\n",
    "    :param train: train data\n",
    "    :param test: test data\n",
    "    :return: test_accuracy, train_accuracy\n",
    "    \"\"\"\n",
    "    print(\"classify_random_forest\", end=\"\")\n",
    "    print(\" Model Fitting....\")\n",
    "    classifier = RandomForestClassifier(n_estimators=200)\n",
    "    classifier.fit(train.ix[:, 0:30], train.ix[:, 30:].values.ravel())\n",
    "    pickle.dump(classifier, open(\"RandomForest.dat\", \"wb\"))\n",
    "    res = classifier.predict(test.ix[:, 0:30])\n",
    "    accuracy = metrics.accuracy_score(test.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on testing data using model and accuracy: \", accuracy)\n",
    "    res = classifier.predict(train.ix[:, 0:30])\n",
    "    train_accuracy = metrics.accuracy_score(train.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on Training data using model and accuracy: \", train_accuracy)\n",
    "    return accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gaussian_nb(train, test):\n",
    "    \"\"\"\n",
    "    classify using naive bayes\n",
    "    :param train: train data\n",
    "    :param test: test data\n",
    "    :return: test_accuracy, train_accuracy\n",
    "    \"\"\"\n",
    "    print(\"classify_gaussian_nb\", end=\"\")\n",
    "    print(\" Model Fitting....\")\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train.ix[:, 0:30], train.ix[:, 30:].values.ravel())\n",
    "    pickle.dump(classifier, open(\"GaussianNB.dat\", \"wb\"))\n",
    "    res = classifier.predict(test.ix[:, 0:30])\n",
    "    accuracy = metrics.accuracy_score(test.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on testing data using model and accuracy: \", accuracy)\n",
    "    res = classifier.predict(train.ix[:, 0:30])\n",
    "    train_accuracy = metrics.accuracy_score(train.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on Training data using model and accuracy: \", train_accuracy)\n",
    "    return accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gradient_boosting_classifier(train, test):\n",
    "    \"\"\"\n",
    "    classify using gradient boosting\n",
    "    :param train: train data\n",
    "    :param test: test data\n",
    "    :return: test_accuracy, train_accuracy\n",
    "    \"\"\"\n",
    "    print(\"classify_gradient_boosting_classifier\", end=\"\")\n",
    "    print(\" Model Fitting....\")\n",
    "    classifier = GradientBoostingClassifier()\n",
    "    classifier.fit(train.ix[:, 0:30], train.ix[:, 30:].values.ravel())\n",
    "    pickle.dump(classifier, open(\"gradient.dat\", \"wb\"))\n",
    "    res = classifier.predict(test.ix[:, 0:30])\n",
    "    accuracy = metrics.accuracy_score(test.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on testing data using model and accuracy: \", accuracy)\n",
    "    res = classifier.predict(train.ix[:, 0:30])\n",
    "    train_accuracy = metrics.accuracy_score(train.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on Training data using model and accuracy: \", train_accuracy)\n",
    "    return accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_logistic_regression(train, test):\n",
    "    \"\"\"\n",
    "    classify using logistic regression\n",
    "    :param train: train data\n",
    "    :param test: test data\n",
    "    :return: test_accuracy, train_accuracy\n",
    "    \"\"\"\n",
    "    print(\"classify_logistic_regression\", end=\"\")\n",
    "    print(\" Model Fitting....\")\n",
    "    classifier = LogisticRegression(multi_class='ovr', C=100)\n",
    "    classifier.fit(train.ix[:, 0:30], train.ix[:, 30:].values.ravel())\n",
    "    pickle.dump(classifier, open(\"Logistic.dat\", \"wb\"))\n",
    "    res = classifier.predict(test.ix[:, 0:30])\n",
    "    accuracy = metrics.accuracy_score(test.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on testing data using model and accuracy: \", accuracy)\n",
    "    res = classifier.predict(train.ix[:, 0:30])\n",
    "    train_accuracy = metrics.accuracy_score(train.ix[:, 30:], res, normalize=True)\n",
    "    print(\"Tested on Training data using model and accuracy: \", train_accuracy)\n",
    "    return accuracy, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    main function performing the reading, fitting and classifying\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    read_input_split_data(all_data)\n",
    "    train, test = read_train_test_csv()\n",
    "    print(\"train shape: \\n\", train.shape)\n",
    "    print(\"test shape: \\n\", test.shape)\n",
    "    # print(\"train keys: \\n\", train.keys())\n",
    "    # print(train.head(1))\n",
    "    # print(\"shape of 30 columns train:\", train.ix[:, 30:].shape)\n",
    "    performances = []\n",
    "    performances.append([\"Naive Bayes\", classify_gaussian_nb(train, test)])\n",
    "    performances.append([\"Random Forest\", classify_random_forest(train, test)])\n",
    "    performances.append([\"Logistic Regression\", classify_logistic_regression(train, test)])\n",
    "    performances.append([\"Gradient Boosting\", classify_gradient_boosting_classifier(train, test)])\n",
    "    for val in performances:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    },
    {
     "ename": "OutOfBoundsDatetime",
     "evalue": "Out of bounds nanosecond timestamp: 2557-08-15 00:00:00",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1979\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1980\u001b[0m             \u001b[0;31m# If tzaware, these values represent unix timestamps, so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-ebf6e9c56af0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mread_input_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_train_test_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train shape: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-29d8051a8a48>\u001b[0m in \u001b[0;36mread_input_split_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date_time\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date_time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"srch_ci\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"srch_ci\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"srch_ci_year\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"srch_ci\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"srch_ci_month\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"srch_ci\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    772\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0munique_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mcache_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, box, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mallow_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_parsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m             \u001b[0myearfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m         )\n\u001b[1;32m   1977\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/np_datetime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.np_datetime.check_dts_bounds\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfBoundsDatetime\u001b[0m: Out of bounds nanosecond timestamp: 2557-08-15 00:00:00"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
